---
title: "Model Evaluations"
author: "Alexander Abinader"
date: "2024-02-25"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(jsonlite)

```


First, we'll read in the labeled training data. We'll also count how many of each sentiment there are (pos, neu, neg).

```{r}

# Read the labeled training data and count the frequency of each sentiment
labeled_reviews <- read.csv("../data/reviews_labeled.csv")
positives <- sum(labeled_reviews$sentiment == "pos")
neutrals <- sum(labeled_reviews$sentiment == "neu")
negatives <- sum(labeled_reviews$sentiment == "neg")

print(sprintf("Positives Reviews: %d, Neutral Reviews: %d, Negative Reviews: %d, Total: %d"
              , positives, neutrals, negatives, nrow(labeled_reviews)))

```

We see that there are many more positive reviews than there are neutral or negative. This is not a dealbreaker but could
certainly affect the interpretation of our results and the validity of our models. For example, we can just have a model
that doesn't do anything and guesses "positive" every time. This would be over 70% accurate given the sheer number of
positive reviews. We'll keep this in mind moving forward.

Let's first assess the TextBlob model which I predict to be the worst one. We'll do some testing to find which cutoffs are the
best for yielding good results. From the website, the sentiment feature of TextBlob returns a tuple with a polarity and subjectivity score. The polarity score lies in the interval [-1, 1] and the subjectivity score lies in the interval [0, 1]. 0 is more objective and 1 is more subjective. This is pretty much all the TextBlob website says so I did some additional digging on Stack Overflow and Github.

It turns out if you toggle a parameter that it uses a Naive Bayes model trained on a dataset of movie reviews. As I mentioned, I wanted to test out just a simple rule-based model and did not use this feature. By default, it just averages the scores of each word in the string using a massive online database of hand-tagged adjectives. I linked that database in my references so I can look at it in more detail later on.

```{r}

# https://textblob.readthedocs.io/en/dev/quickstart.html
textblob <- read.csv("tblob.csv")

```

```{r}

# Read the json format sentiment
parsed_sentiment <- lapply(textblob$sentiment, function(cell) {

  json_val <- gsub("'", "\"", cell)
  fromJSON(json_val)
  
})

# Extract polarity and subjectivity
textblob$polarity <- sapply(parsed_sentiment, function(x) x$polarity)
textblob$subjectivity <- sapply(parsed_sentiment, function(x) x$subjectivity)

```

Now I'm going to calculate a sentiment score. I think the followin formula is a good start: $sentiment = polarity \times (1 - subjectivity)$. This makes sense because if the score is perfectly objective then we will use the polarity as is and if it the
score is entirely subjective then it's basically a toss up bring the score to a neutral zero. I'll also just try using polarity straight up.

```{r}

# Calculate a new sentiment score
textblob$sentiment_score <- textblob$polarity # * (1 - textblob$subjectivity)
boxplot(textblob$sentiment_score, ylab = "Sentiment Score", main = "Boxplot of Textblob Scores", col = "blue")

```

```{r}

textblob$label <- ifelse(textblob$sentiment_score < -0.05, "neg",
                    ifelse(textblob$sentiment_score < 0.05, "neu", "pos"))

textblob$true_label <- labeled_reviews$sentiment
textblob$correct <- textblob$label == textblob$true_label
accuracy_textblob <- (sum(textblob$correct) / length(textblob$correct)) * 100
print(accuracy_textblob)

```

Using the the complex formula gave an accuracy of about 60% which is not very good. Using polarity straight up had
an accuracy of about 65% which is a little bit better. Let's move on to VADER which I think will do a better job.

```{r}

vader <- read.csv("vader.csv")

```

VADER is fairly more complicated. It still uses a rule based approach but has a lot more steps and considerations. I skimmed
through the source code on Github here: https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py

```{r}

vader$sentiment_score <- sapply(vader$sentiment, function(x) {
  
  corrected_json <- gsub("'", "\"", x)
  parsed <- fromJSON(corrected_json)
  return(parsed$compound)
  
})

boxplot(vader$sentiment_score, ylab = "Sentiment Score", main = "Boxplot of VADER Scores", col = "red")

```

```{r}

vader$label <- ifelse(vader$sentiment_score < -0.05, "neg",
                    ifelse(vader$sentiment_score < 0.05, "neu", "pos"))

vader$true_label <- labeled_reviews$sentiment
vader$correct <- vader$label == vader$true_label
accuracy_vader <- (sum(vader$correct) / length(vader$correct)) * 100
print(accuracy_vader)

```

Using cutoffs of -0.05 and 0.05 the VADER model achieved an accuracy of about 75%. That's pretty good! Now let's dig
in to the RoBERTa model, which I anticipate will be the best of the pre built models. It uses deep learning (neural nets)
and is pretrained on millions of tweets.

```{r}

roberta <- read.csv("roberta.csv")

```

RoBERTa outputs a list of probabilities [neg, neu, pos] for how the string belongs to each classification label. The
most obvious approach would be to take the highest probability and assign that as the classification.

```{r}

convert_to_numeric_vector <- function(x) {
  
  nums <- gsub("\\[|\\]", "", x)
  num_vec <- as.numeric(unlist(strsplit(nums, ",\\s*")))
  return(num_vec)
}

roberta$sentiment <- lapply(roberta$sentiment, convert_to_numeric_vector)

get_sentiment_label <- function(probabilities) {
  
  labels <- c("neg", "neu", "pos")
  max_prob_index <- which.max(probabilities)
  return(labels[max_prob_index])
}

roberta$label <- sapply(roberta$sentiment, get_sentiment_label)

```

```{r}

roberta$true_label <- labeled_reviews$sentiment
roberta$correct <- roberta$label == roberta$true_label
accuracy_roberta <- (sum(roberta$correct) / length(roberta$correct)) * 100
print(accuracy_roberta)

```

Finally, the RoBERTa model achieved roughly 77% accuracy. Pretty good!

